{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import lib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cài đặt các gói cần thiết của thư viện bằng file pip install requirement.txt\n",
    "sau đó import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save history"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cài đặt một số hàm cần thiết như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point_landmark_toget = [0, 11 ,12 ,13, 14,15 ,16, 19, 20, 23, 24]\n",
    "# hàm để lấy 24 tọa độ khung xương\n",
    "def get_frame_landmarks(results):\n",
    "    size_landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark[:23]]).flatten() if results.pose_landmarks else np.zeros(4*23)\n",
    "    return size_landmarks\n",
    "\n",
    "# hàm để dự đoán khung xương\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "# hàm để chuyển đổi tọa độ khung xương và convert sang dạng phù hợp để vẽ lên ảnh\n",
    "def get_scaled_landmarks(landmarks, dimenson):\n",
    "    landmarks_2d = []\n",
    "    if dimenson == '2d':\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x*1280), int(landmark.y*720)\n",
    "            landmarks_2d.append([x, y])\n",
    "        return landmarks_2d\n",
    "\n",
    "# hàm hiển thị fps\n",
    "def show_fps(image, prev_frame_time):\n",
    "    new_frame_time = time.time()\n",
    "    fps = int(1/(new_frame_time-prev_frame_time))\n",
    "    cv2.putText(image, f\"fps: {fps}\", (1000, 700), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return new_frame_time\n",
    "        \n",
    "# hàm hiển thị vẽ khung xương\n",
    "def draw_landmark_on_image( results, image):\n",
    "    lmks = results.pose_landmarks.landmark\n",
    "    \n",
    "    pose_landmarks = [lmks[0], lmks[1], lmks[2], lmks[3], lmks[4], lmks[5], lmks[6], lmks[7], lmks[8], lmks[9], lmks[10], lmks[11], lmks[12], lmks[13], lmks[14], lmks[15], lmks[16], lmks[17], lmks[18], lmks[19], lmks[20], lmks[21], lmks[22], lmks[23], lmks[24]] \n",
    "    pose_landmarks = get_scaled_landmarks(pose_landmarks, '2d')\n",
    "  \n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[4]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[1]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[4]), tuple(pose_landmarks[6]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[6]), tuple(pose_landmarks[8]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[3]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[3]), tuple(pose_landmarks[7]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[9]), tuple(pose_landmarks[10]), (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[12]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[23]), (255, 205, 0), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[23]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "        \n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[16]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[22]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[18]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[18]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[21]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[19]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[19]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "    \n",
    "    \n",
    "    for lm in pose_landmarks:\n",
    "        \n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 5, (255,255,255), 4)\n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 4, (255,255,141), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw skeleton on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo thư viện mediapipe và dự đoán trên 1 ảnh bất kỳ\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "frame = cv2.imread(\"fpt_1.png\")\n",
    "frame = cv2.resize(frame,(1300,750)) \n",
    "frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = pose.process(frameRGB)\n",
    "if results.pose_landmarks:\n",
    "    # Ghi nhận thông số khung xương\n",
    "    # Vẽ khung xương lên ảnh\n",
    "    draw_landmark_on_image( results, frame)\n",
    "\n",
    "    lm = get_frame_landmarks(results)\n",
    "\n",
    "    print(lm)\n",
    "cv2.imshow(\"image\", frame)\n",
    "cv2.imwrite(\"skeleton_fpt.png\", frame)\n",
    "# cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get skeleton from video and save them into csv file\n",
    "trích xuất khung xương và tạo data set để huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './data/'\n",
    "path_video = os.path.join(path_data,'train')\n",
    "path_csv = os.path.join(path_data,'landmark','train')\n",
    "name_actions = ['cheat','non_cheat']\n",
    "\n",
    "# Khởi tạo thư viện mediapipe\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "prev_frame_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_action in name_actions:\n",
    "    direction_path = os.path.join(path_video,name_action)\n",
    "    lm_list = []\n",
    "    frame_num = 0\n",
    "    \n",
    "    for id_sequence, video_name in enumerate(os.listdir(direction_path)):\n",
    "        video_path = os.path.join(direction_path, video_name)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Video end\")\n",
    "                break\n",
    "\n",
    "            frame_num += 1\n",
    "            print(frame_num)\n",
    "            \n",
    "            frame = cv2.resize(frame,(1300,750)) \n",
    "            frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = pose.process(frameRGB)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                # Ghi nhận thông số khung xương\n",
    "                # Vẽ khung xương lên ảnh\n",
    "                draw_landmark_on_image( results, frame)\n",
    "                \n",
    "                lm = get_frame_landmarks(results)\n",
    "                \n",
    "                lm_list.append(lm)\n",
    "                print(lm)\n",
    "#             prev_frame_time = show_fps(frame, prev_frame_time)\n",
    "#             cv2.imshow(\"image\", frame)\n",
    "#             if cv2.waitKey(1) == ord('q'):\n",
    "#                 break\n",
    "  \n",
    "#         cap.release()\n",
    "        \n",
    "    df  = pd.DataFrame(lm_list)\n",
    "    \n",
    "#     name_history = 'cheat_B.txt'\n",
    "#     csv_per_action = os.path.join(path_csv,name_history)\n",
    "#     df.to_csv(csv_per_action)\n",
    "    \n",
    "    name_history = name_action +'.csv'\n",
    "    csv_per_action = os.path.join(path_csv,name_history)\n",
    "    with open(csv_per_action, mode='w') as f:\n",
    "        df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './data/'\n",
    "path_video = os.path.join(path_data,'test')\n",
    "path_csv = os.path.join(path_data,'landmark','test')\n",
    "name_actions = ['cheat','non_cheat']\n",
    "\n",
    "# Khởi tạo thư viện mediapipe\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "prev_frame_time = 0\n",
    "\n",
    "for name_action in name_actions:\n",
    "    direction_path = os.path.join(path_video,name_action)\n",
    "    lm_list = []\n",
    "    frame_num = 0\n",
    "    \n",
    "    for id_sequence, video_name in enumerate(os.listdir(direction_path)):\n",
    "        video_path = os.path.join(direction_path, video_name)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Video end\")\n",
    "                break\n",
    "\n",
    "            frame_num += 1\n",
    "            print(frame_num)\n",
    "            \n",
    "            frame = cv2.resize(frame,(1300,750)) \n",
    "            frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frameRGB)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                # Ghi nhận thông số khung xương\n",
    "                # Vẽ khung xương lên ảnh\n",
    "#                 draw_landmark_on_image( results, frame)\n",
    "                \n",
    "                lm = get_frame_landmarks(results)\n",
    "                \n",
    "                lm_list.append(lm)\n",
    "                print(lm)\n",
    "#             prev_frame_time = show_fps(frame, prev_frame_time)\n",
    "#             cv2.imshow(\"image\", frame)\n",
    "#             if cv2.waitKey(1) == ord('q'):\n",
    "#                 break\n",
    "  \n",
    "#         cap.release()\n",
    "        \n",
    "    df  = pd.DataFrame(lm_list)\n",
    "    \n",
    "#     name_history = 'cheat_B.txt'\n",
    "#     csv_per_action = os.path.join(path_csv,name_history)\n",
    "#     df.to_csv(csv_per_action)\n",
    "    \n",
    "    name_history = name_action +'.csv'\n",
    "    csv_per_action = os.path.join(path_csv,name_history)\n",
    "    with open(csv_per_action, mode='w') as f:\n",
    "        df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file csv\n",
    "\n",
    "#load dataset cho việc huấn luyện mô hình\n",
    "cheat = pd.read_csv(\"./data/New folder/train/cheat.csv\")\n",
    "non_cheat = pd.read_csv(\"./data/New folder/train/non_cheat.csv\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "no_of_timesteps = 10\n",
    "\n",
    "dataset = cheat.iloc[:,1:].values\n",
    "n_sample = len(dataset)\n",
    "for i in range(no_of_timesteps, n_sample):\n",
    "    X.append(dataset[i-no_of_timesteps:i,:])\n",
    "    y.append(1)\n",
    "                        \n",
    "dataset = non_cheat.iloc[:,1:].values\n",
    "n_sample = len(dataset)\n",
    "for i in range(no_of_timesteps, n_sample):\n",
    "    X.append(dataset[i-no_of_timesteps:i,:])\n",
    "    y.append(0)\n",
    "    \n",
    "    \n",
    "# test\n",
    "cheat = pd.read_csv(\"./data/New folder/test/cheat.csv\")\n",
    "non_cheat = pd.read_csv(\"./data/New folder/test/non_cheat.csv\")\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "no_of_timesteps = 10\n",
    "\n",
    "dataset = cheat.iloc[:,1:].values\n",
    "n_sample = len(dataset)\n",
    "for i in range(no_of_timesteps, n_sample):\n",
    "    X_test.append(dataset[i-no_of_timesteps:i,:])\n",
    "    y_test.append(1)\n",
    "                        \n",
    "dataset = non_cheat.iloc[:,1:].values\n",
    "n_sample = len(dataset)\n",
    "for i in range(no_of_timesteps, n_sample):\n",
    "    X_test.append(dataset[i-no_of_timesteps:i,:])\n",
    "    y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X), np.array(y)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_val, y_val = X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using LSTM architecture for training\n",
    "\n",
    "def LSTM():\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    layer = keras.layers.LSTM(\n",
    "        32, activation=tf.nn.relu6, return_sequences=True)(inputs)\n",
    "    layer = keras.layers.Dropout(0.2)(layer)\n",
    "    layer = keras.layers.LSTM(32, activation=tf.nn.relu6)(layer)\n",
    "    layer = keras.layers.Dropout(0.2)(layer)\n",
    "    layer = keras.layers.Dense(16, activation=tf.nn.relu6)(layer)\n",
    "    layer = keras.layers.Dropout(0.2)(layer)\n",
    "\n",
    "    outputs = keras.layers.Dense(units = 1, activation=\"sigmoid\")(layer)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=\"adam\", metrics = ['accuracy'], loss = \"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def shalowCNN():\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    layer = keras.layers.Dense(16, activation=tf.nn.relu6)(inputs)\n",
    "#     layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "#     layer = keras.layers.Dense(128, activation=tf.nn.relu6)(layer)\n",
    "    layer = keras.layers.Flatten()(layer)\n",
    "    layer = keras.layers.Dropout(0.2)(layer)\n",
    "    outputs = keras.layers.Dense(units = 1, activation=\"sigmoid\")(layer)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=\"adam\", metrics = ['accuracy'], loss = \"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_confusion_matrix(plot_confusion_matrix_path,cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "        \"\"\"Plots the confusion matrix.\"\"\"\n",
    "        if normalize:\n",
    "          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "          print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "          print('Confusion matrix, without normalization')\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=55)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "          plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        fig.savefig(plot_confusion_matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "model = shalowCNN()\n",
    "\n",
    "figure_data_path = './figures/shalowCNN/3'\n",
    "\n",
    "checkpoint_path = os.path.join(figure_data_path,\"weights.best.hdf5\")\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                          patience = 10)\n",
    "time_start = time.time()\n",
    "history = model.fit(X_train,y_train ,\n",
    "          batch_size=64,\n",
    "          epochs = 500,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[checkpoint,earlystopping])\n",
    "\n",
    "time_end = time.time()\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "loss_test,accuracy_test = model.evaluate(X_test, y_test)\n",
    "print('LOSS TEST: ', loss_test)\n",
    "print(\"ACCURACY TEST: \", accuracy_test)\n",
    "\n",
    "\n",
    "loss_train, accuracy_train = model.evaluate(X_train, y_train)\n",
    "print('LOSS TRAIN: ', loss_train)\n",
    "print(\"ACCURACY TRAIN: \", accuracy_train)\n",
    "\n",
    "\n",
    "data_eval = 'LOSS TEST: '+ str(loss_test) + ' \\n ACCURACY TEST: '+ str(accuracy_test) + '\\n' +'LOSS TRAIN: '+ str(loss_train) +'\\n ACCURACY TRAIN: '+ str(accuracy_train) +\"\\n time : \"+str((time_end - time_start)/60)\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "name_history = 'history.csv'\n",
    "path_history = os.path.join(figure_data_path , name_history)\n",
    "with open(path_history, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# write EVAUATION into txt\n",
    "Name_f = 'EVAUATION.txt'\n",
    "# f = open('./figure/CNN/train_1st/EVAUATION_1st.txt','w+')\n",
    "\n",
    "path_s= os.path.join(figure_data_path , Name_f)\n",
    "\n",
    "with open(path_s, mode='w') as f:\n",
    "    f.writelines(data_eval)\n",
    "f.close()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(figure_data_path , 'acc_his.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(figure_data_path , 'loss_his.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_frame_landmarks(results):\n",
    "    size_landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark[:23]]).flatten() if results.pose_landmarks else np.zeros(4*23)\n",
    "    return size_landmarks\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def get_scaled_landmarks(landmarks, dimenson):\n",
    "    landmarks_2d = []\n",
    "    if dimenson == '2d':\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x*1280), int(landmark.y*720)\n",
    "            landmarks_2d.append([x, y])\n",
    "        return landmarks_2d\n",
    "\n",
    "def draw_landmark_on_image( results, image):\n",
    "    lmks = results.pose_landmarks.landmark\n",
    "    \n",
    "    pose_landmarks = [lmks[0], lmks[1], lmks[2], lmks[3], lmks[4], lmks[5], lmks[6], lmks[7], lmks[8], lmks[9], lmks[10], lmks[11], lmks[12], lmks[13], lmks[14], lmks[15], lmks[16], lmks[17], lmks[18], lmks[19], lmks[20], lmks[21], lmks[22], lmks[23], lmks[24]] \n",
    "    pose_landmarks = get_scaled_landmarks(pose_landmarks, '2d')\n",
    "  \n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[4]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[1]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[4]), tuple(pose_landmarks[6]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[6]), tuple(pose_landmarks[8]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[3]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[3]), tuple(pose_landmarks[7]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[9]), tuple(pose_landmarks[10]), (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[12]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[23]), (255, 205, 0), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[23]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "        \n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[16]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[22]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[18]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[18]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[21]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[19]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[19]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "    \n",
    "    \n",
    "    for lm in pose_landmarks:\n",
    "        \n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 5, (255,255,255), 4)\n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 4, (255,255,141), -1)\n",
    "\n",
    "        \n",
    "\n",
    "def show_fps(image, prev_frame_time):\n",
    "    new_frame_time = time.time()\n",
    "    fps = int(1/(new_frame_time-prev_frame_time))\n",
    "    cv2.putText(image, f\"fps: {fps}\", (1000, 700), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return new_frame_time\n",
    "\n",
    "def draw_class_on_image(label, img):\n",
    "    \n",
    "    fontColor = (255, 255 , 0)\n",
    "    if label == \"non_cheat\":\n",
    "        fontColor = (255, 255 , 0)\n",
    "    elif label == \"cheat\":\n",
    "        fontColor = (0, 69, 255)\n",
    "        cv2.rectangle(img=img,\n",
    "            pt1=(20, 20),\n",
    "            pt2=(1260,700),\n",
    "            color=(0, 144, 255),\n",
    "            thickness=3,\n",
    "            lineType=2,\n",
    "            shift=0)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (950, 550)\n",
    "    fontScale = 1.5\n",
    "    \n",
    "    thickness = 3\n",
    "    lineType = 2\n",
    "    cv2.putText(img, label,\n",
    "                bottomLeftCornerOfText,\n",
    "                font,\n",
    "                fontScale,\n",
    "                fontColor,\n",
    "                thickness,\n",
    "                lineType)\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect(model, lm_list):\n",
    "    global label\n",
    "    lm_list = np.array(lm_list)\n",
    "    lm_list = np.expand_dims(lm_list, axis=0)\n",
    "    print(lm_list.shape)\n",
    "    results = model.predict(lm_list)\n",
    "    print(results)\n",
    "    if results[0][0] > 0.5:\n",
    "        label = \"cheat\"\n",
    "    else:\n",
    "        label = \"non_cheat\"\n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Warmup....\"\n",
    "n_time_steps = 10\n",
    "lm_list = []\n",
    "\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# model = tf.keras.models.load_model(\"./model.h5\")\n",
    "model.load_weights(\"./figures/shalowCNN/2/weights.best.hdf5\")\n",
    "# cap = cv2.VideoCapture('http://192.168.79.14:8080/video')\n",
    "cap = cv2.VideoCapture(\"test_non.mp4\")\n",
    "\n",
    "# Khởi tạo thư viện mediapipe\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "prev_frame_time = 0\n",
    "\n",
    "\n",
    "i = 0\n",
    "warmup_frames = 0\n",
    "lm_list = []\n",
    "while True:\n",
    "\n",
    "    success, frame = cap.read()  \n",
    "    \n",
    "    image, results = mediapipe_detection(frame, pose)\n",
    "    \n",
    "    frame = cv2.resize(frame,(1280,720))\n",
    "    \n",
    "    i = i+1\n",
    "    if i > warmup_frames:\n",
    "        print(\"Start detect....\")\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "                # Ghi nhận thông số khung xương\n",
    "                # Vẽ khung xương lên ảnh\n",
    "                draw_landmark_on_image( results, frame)\n",
    "\n",
    "                lm = get_frame_landmarks(results)\n",
    "\n",
    "                \n",
    "                lm_list.append(lm)\n",
    "                print(lm)\n",
    "        \n",
    "            \n",
    "                if len(lm_list) == n_time_steps:\n",
    "                    t1 = threading.Thread(target=detect, args=(model, lm_list,))\n",
    "                    t1.start()\n",
    "                    lm_list = []\n",
    "\n",
    "                print(lm)\n",
    "            \n",
    "    frame = draw_class_on_image(label, frame)\n",
    "    prev_frame_time = show_fps(frame, prev_frame_time)\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expand web cam and mobie cam (detect on face and body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pose detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_frame_landmarks(results):\n",
    "    size_landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark[:23]]).flatten() if results.pose_landmarks else np.zeros(4*23)\n",
    "    return size_landmarks\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def get_scaled_landmarks(landmarks, dimenson):\n",
    "    landmarks_2d = []\n",
    "    if dimenson == '2d':\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x*1280), int(landmark.y*720)\n",
    "            landmarks_2d.append([x, y])\n",
    "        return landmarks_2d\n",
    "\n",
    "def draw_landmark_on_image( results, image):\n",
    "    lmks = results.pose_landmarks.landmark\n",
    "    pose_landmarks = [lmks[0], lmks[11], lmks[12], lmks[13], lmks[14], lmks[15], lmks[16], lmks[23], lmks[24], lmks[19], lmks[20]] \n",
    "    pose_landmarks = get_scaled_landmarks(pose_landmarks, '2d')\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[4]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[0]), tuple(pose_landmarks[1]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[4]), tuple(pose_landmarks[6]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[6]), tuple(pose_landmarks[8]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[3]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[3]), tuple(pose_landmarks[7]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[9]), tuple(pose_landmarks[10]), (255, 255, 255), 2)\n",
    "\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[12]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[11]), tuple(pose_landmarks[23]), (255, 205, 0), 2)\n",
    "\n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[23]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[24]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "\n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[16]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[14]), tuple(pose_landmarks[12]), (0, 144, 255), 2)\n",
    "\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[22]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[16]), tuple(pose_landmarks[18]), (0, 144, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[18]), tuple(pose_landmarks[20]), (0, 144, 255), 2)\n",
    "\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[13]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[21]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[19]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[15]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[19]), tuple(pose_landmarks[17]), (255, 205, 0), 2)\n",
    "\n",
    "\n",
    "    for lm in pose_landmarks:\n",
    "\n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 5, (255,255,255), 4)\n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 4, (255,255,141), -1)\n",
    "\n",
    "def show_fps(image, prev_frame_time):\n",
    "    new_frame_time = time.time()\n",
    "    fps = int(1/(new_frame_time-prev_frame_time))\n",
    "    cv2.putText(image, f\"fps: {fps}\", (1000, 700), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return new_frame_time\n",
    "\n",
    "def draw_class_on_image(label, img):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (10, 30)\n",
    "    fontScale = 1\n",
    "    fontColor = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    lineType = 2\n",
    "    cv2.putText(img, label,\n",
    "                bottomLeftCornerOfText,\n",
    "                font,\n",
    "                fontScale,\n",
    "                fontColor,\n",
    "                thickness,\n",
    "                lineType)\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect(model, lm_list):\n",
    "    global label\n",
    "    lm_list = np.array(lm_list)\n",
    "    lm_list = np.expand_dims(lm_list, axis=0)\n",
    "    print(lm_list.shape)\n",
    "    results = model.predict(lm_list)\n",
    "    print(results)\n",
    "    if results[0][0] > 0.5:\n",
    "        label = \"cheat\"\n",
    "    else:\n",
    "        label = \"non_cheat\"\n",
    "    return label\n",
    "\n",
    "def body_pose_detect(frame1, i, prev_frame_time, model,lm_list):\n",
    "    image, results = mediapipe_detection(frame1, pose)\n",
    "    \n",
    "    frame1 = cv2.resize(frame1,(1280,720))\n",
    "\n",
    "    i = i+1\n",
    "    if i > warmup_frames:\n",
    "        print(\"Start detect....\")\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                draw_landmark_on_image( results, frame1)\n",
    "                lm = get_frame_landmarks(results)\n",
    "                lm_list.append(lm)\n",
    "                print(lm)\n",
    "\n",
    "\n",
    "                if len(lm_list) == n_time_steps:\n",
    "                    t1 = threading.Thread(target=detect, args=(model, lm_list,))\n",
    "                    t1.start()\n",
    "                    lm_list = []\n",
    "\n",
    "                print(lm)\n",
    "\n",
    "    frame1 = draw_class_on_image(label, frame1)\n",
    "    prev_frame_time = show_fps(frame1, prev_frame_time)\n",
    "    \n",
    "    return frame1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head pose & face detect"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "các hàm hỗ trợ việc phát hiện chuyển động của khuôn mặt và chuyển động của đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "faceMesh = mpFaceMesh.FaceMesh(max_num_faces=1)\n",
    "mouth_movements = []\n",
    "iw, ih =  720, 580\n",
    "warn_mouth = \"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_mouth_movement(upper_mouth, bottom_mouth):\n",
    "    distance = abs(int(upper_mouth.y * ih) - int(bottom_mouth.y * ih))\n",
    "    if distance > 8:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def warning(warnings):\n",
    "    count = 0\n",
    "    for warning in warnings:\n",
    "        if warning == 1:\n",
    "            count = count + 1;\n",
    "    if count >= 5:\n",
    "        return \"warning: Talking\"\n",
    "    else : \n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def face_detect(image, results):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])       \n",
    "            \n",
    "            # Convert it to the NumPy array\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "\n",
    "            # Convert it to the NumPy array\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                    [0, focal_length, img_w / 2],\n",
    "                                    [0, 0, 1]])\n",
    "\n",
    "            # The distortion parameters\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve PnP\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "            # Get rotational matrix\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            # Get angles\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            # Get the y rotation degree\n",
    "            x = angles[0] * 360\n",
    "            y = angles[1] * 360\n",
    "            z = angles[2] * 360\n",
    "          \n",
    "\n",
    "            if y < -10:\n",
    "                text = \"Looking Left\"\n",
    "            elif y > 10:\n",
    "                text = \"Looking Right\"\n",
    "            elif x < -10:\n",
    "                text = \"Looking Down\"\n",
    "            elif x > 10:\n",
    "                text = \"Looking Up\"\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "\n",
    "            cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                                cv2.LINE_AA)\n",
    "            cv2.putText(image, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            \n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    \n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "            \n",
    "        image = cv2.resize(image,(720,580))\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        # Draw the face mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "                lmks = face_landmarks.landmark\n",
    "\n",
    "                mouth_movement = get_mouth_movement(lmks[13], lmks[14])\n",
    "                mouth_movements.append(mouth_movement)\n",
    "                X = (int(lmks[113].x*image.shape[1]),int(lmks[113].y*image.shape[0]))\n",
    "                Y = (int(lmks[434].x*image.shape[1]),int(lmks[434].y*image.shape[0]))\n",
    "#                 print(image.shape)\n",
    "                print(X)\n",
    "                print(Y)\n",
    "                cv2.rectangle(image, Y, X,(179,222,245),-1)\n",
    "                if len(mouth_movements) == 16:\n",
    "                    warn_mouth = warning(mouth_movements)\n",
    "                    mouth_movements = []\n",
    "                    \n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_tesselation_style())\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_contours_style())\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_iris_connections_style())\n",
    "            \n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        if warn_mouth != \"\":\n",
    "            image = cv2.putText(image, str(warn_mouth), (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 2,cv2.LINE_AA)\n",
    "        image = face_detect(image , results)\n",
    "\n",
    "#         cv2.imshow('Head Pose Estimation', image)\n",
    "\n",
    "        cv2.imshow('MediaPipe Face Mesh',image)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
